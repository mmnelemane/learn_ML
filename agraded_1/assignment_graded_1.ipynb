{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logo.png\" style=\"width: 100px;\"/>\n",
    "<h1><center>Graded Assignment 1</center></h1>\n",
    "\n",
    "<center>Due: 23.12.2020 at 23:59</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to upload:\n",
    "\n",
    "Upload your solution via the VC course. Please upload **one Zip archive** per group. The Zip must contain:\n",
    "* Your solution **notebook** (a **.ipynb** file)\n",
    "* An **images folder** with all your images (keep the size of the images relatively small)\n",
    "* A **data folder** with the datasets (you probably don't have to change anything here)\n",
    "\n",
    "Your Zip should be named after the following scheme:\n",
    "\n",
    "* assignment_graded\\_\"**assignment number**\"\\_solution.zip\n",
    "\n",
    "__NOTE: This assignment will be graded and you can receive bonus points for the exam.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Decision Tree Implementation (10 points)\n",
    "In this first task, we will take a look at an implementation of the ID3 algorithm. \n",
    "\n",
    "For this task, we will use the mushroom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poisonous</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>l</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>g</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   poisonous cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
       "0          1         x           s         n       t    p               f   \n",
       "1          0         x           s         y       t    a               f   \n",
       "2          0         b           s         w       t    l               f   \n",
       "3          1         x           y         w       t    p               f   \n",
       "4          0         x           s         g       f    n               f   \n",
       "\n",
       "  gill-spacing gill-size gill-color  ... stalk-surface-below-ring  \\\n",
       "0            c         n          k  ...                        s   \n",
       "1            c         b          k  ...                        s   \n",
       "2            c         b          n  ...                        s   \n",
       "3            c         n          n  ...                        s   \n",
       "4            w         b          k  ...                        s   \n",
       "\n",
       "  stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
       "0                      w                      w         p          w   \n",
       "1                      w                      w         p          w   \n",
       "2                      w                      w         p          w   \n",
       "3                      w                      w         p          w   \n",
       "4                      w                      w         p          w   \n",
       "\n",
       "  ring-number ring-type spore-print-color population habitat  \n",
       "0           o         p                 k          s       u  \n",
       "1           o         p                 n          n       g  \n",
       "2           o         p                 n          n       m  \n",
       "3           o         p                 k          s       u  \n",
       "4           o         e                 n          a       g  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"data/UCI_mushroom_data.csv\", sep=\",\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper methods are needed for the implementation of the **ID3 Algorithm**. <br>\n",
    "__Do not change them.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for calculating the multiclass entropy of a dataset and a target attribute\n",
    "def multiclass_entropy(data, target_attribute):\n",
    "    labels = data[target_attribute]\n",
    "    num_entries = len(labels)\n",
    "    classes = labels.unique()\n",
    "    \n",
    "    entropy = 0\n",
    "    for c in classes:\n",
    "        n_selection = len(labels[labels == c])\n",
    "        if n_selection == 0:\n",
    "            entropy += 0\n",
    "        else:\n",
    "            p = n_selection / num_entries\n",
    "            entropy += -p*np.log2(p)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Method for calculating the information gain given a dataset, an attribute to check for and the target attribute\n",
    "def information_gain(data, attribute, target_attribute):\n",
    "    system_entropy = multiclass_entropy(data, target_attribute)\n",
    "    num_entries = len(data)\n",
    "    \n",
    "    relative_entropy = 0\n",
    "    for v in data[attribute].unique():\n",
    "        sv = data[data[attribute] == v]\n",
    "        relative_frequency = len(sv) / num_entries\n",
    "        entropy_sv = multiclass_entropy(sv, target_attribute)\n",
    "        relative_entropy += relative_frequency * entropy_sv\n",
    "    \n",
    "    return system_entropy - relative_entropy\n",
    "\n",
    "# structural classes needed for the ID3 implementation\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.value = \"\"\n",
    "        self.edges = []\n",
    "\n",
    "class Edge:\n",
    "    def __init__(self):\n",
    "        self.label = \"\"\n",
    "        self.node = None\n",
    "        \n",
    "# Method for printing a tree.\n",
    "def print_tree(root):\n",
    "    print(print_tree_at_layer(root, 0))\n",
    "    \n",
    "# Helper Method for printing a tree.\n",
    "def print_tree_at_layer(tree, layer):\n",
    "    text = str(tree.value)\n",
    "    text += \"\\n\"\n",
    "    if not len(tree.edges) == 0:\n",
    "        for e in tree.edges:\n",
    "            text += \"\\t\" * (layer+1)\n",
    "            text += e.label + \": \" + print_tree_at_layer(e.node, layer + 1)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an implementation ot the **ID3 Algorithm**:\n",
    "The parameters are:\n",
    "- examples: dataset (pandas dataframe)\n",
    "- target_attribute: the name of the target attribute column (str)\n",
    "- attributes: a list of all attributes, column names, of the dataset (list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odor\n",
      "\tp: 1\n",
      "\ta: 0\n",
      "\tl: 0\n",
      "\tn: spore-print-color\n",
      "\t\tk: 0\n",
      "\t\tn: 0\n",
      "\t\tu: 0\n",
      "\t\th: 0\n",
      "\t\tr: 1\n",
      "\t\tw: cap-color\n",
      "\t\t\tn: 0\n",
      "\t\t\ty: 1\n",
      "\t\t\tw: 1\n",
      "\t\t\tg: 0\n",
      "\t\t\te: 0\n",
      "\t\t\tp: 0\n",
      "\t\t\tb: 0\n",
      "\t\t\tc: 0\n",
      "\tf: 1\n",
      "\tc: 1\n",
      "\tm: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def id3(examples, target_attribute, attributes):\n",
    "    root = Node()\n",
    "    \n",
    "    unique_labels = examples[target_attribute].unique()\n",
    "    \n",
    "    # Create leaf node if there is only one unique label left\n",
    "    if len(unique_labels) == 1:\n",
    "        root.value = unique_labels[0]\n",
    "        return root\n",
    "    \n",
    "    # Create leaf node if there are no more attributes left to split on\n",
    "    # the classification is the label with the most occurances\n",
    "    if len(attributes) == 0:\n",
    "        root.value = examples[target_attribute].value_counts().idxmax()\n",
    "        return root\n",
    "    \n",
    "    # Selecting the best attribute for branching (highest information gain)\n",
    "    best_attribute = max(attributes, key=lambda a:information_gain(examples, a, target_attribute))\n",
    "    root.value = best_attribute\n",
    "    \n",
    "    # Create an edge for every unique value of the selected attribute\n",
    "    # We use 'in data[..]' here, because otherwise we'd only create the branches necessary for our subset. If we\n",
    "    # want to use this with new mushrooms down the line, that could cause problems (missing branches)\n",
    "    for v in data[best_attribute].unique():\n",
    "        edge = Edge()\n",
    "        edge.label = v\n",
    "        root.edges.append(edge)\n",
    "        \n",
    "        # Create a subset for each value of the selected attribute\n",
    "        examples_v = examples[examples[best_attribute] == v]\n",
    "        \n",
    "        # Error Branch: Create Leaf when the dataset does not contain any more examples\n",
    "        if len(examples_v) == 0:\n",
    "            child = Node()\n",
    "            child.value = examples[target_attribute].value_counts().idxmax()\n",
    "            edge.node = child;\n",
    "        # Main Branch: Do the next iteration of ID3\n",
    "        else:\n",
    "            new_attributes = [a for a in attributes if a != best_attribute]\n",
    "            edge.node = id3(examples_v, target_attribute, new_attributes)\n",
    "    \n",
    "    return root\n",
    "\n",
    "# Growing the tree and printing it.\n",
    "decision_tree = id3(data, \n",
    "                    'poisonous', \n",
    "                    list(data.drop('poisonous', axis=1).columns))\n",
    "print_tree(decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ID3 algorithm has a lot of parameters to tweak for any given problem. One of these parameters is the selection criterion: How the algorithm chooses which attribute to use next to make a new split. You learned about the gain ratio in the lecture.\n",
    "\n",
    "__Task 1.1 (3 points)__ Extend/Change the code below to change the selection criterion from the attribute with the highest information gain to the attribute with the highest __gain ratio__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_ratio():\n",
    "    pass    \n",
    "\n",
    "\n",
    "def id3_gain_ratio(examples, target_attribute, attributes):\n",
    "    root = Node()\n",
    "    \n",
    "    unique_labels = examples[target_attribute].unique()\n",
    "    \n",
    "    # Create leaf node if there is only one unique label left\n",
    "    if len(unique_labels) == 1:\n",
    "        root.value = unique_labels[0]\n",
    "        return root\n",
    "    \n",
    "    # Create leaf node if there are no more attributes left to split on\n",
    "    # the classification is the label with the most occurances\n",
    "    if len(attributes) == 0:\n",
    "        root.value = examples[target_attribute].value_counts().idxmax()\n",
    "        return root\n",
    "    \n",
    "    # Selecting the best attribute for branching (highest information gain)\n",
    "    best_attribute = max(attributes, key=lambda a:information_gain(examples, a, target_attribute))\n",
    "    root.value = best_attribute\n",
    "    \n",
    "    # Create an edge for every unique value of the selected attribute\n",
    "    # We use 'in data[..]' here, because otherwise we'd only create the branches necessary for our subset. If we\n",
    "    # want to use this with new mushrooms down the line, that could cause problems (missing branches)\n",
    "    for v in data[best_attribute].unique():\n",
    "        edge = Edge()\n",
    "        edge.label = v\n",
    "        root.edges.append(edge)\n",
    "        \n",
    "        # Create a subset for each value of the selected attribute\n",
    "        examples_v = examples[examples[best_attribute] == v]\n",
    "        \n",
    "        # Error Branch: Create Leaf when the dataset does not contain any more examples\n",
    "        if len(examples_v) == 0:\n",
    "            child = Node()\n",
    "            child.value = examples[target_attribute].value_counts().idxmax()\n",
    "            edge.node = child;\n",
    "        # Main Branch: Do the next iteration of ID3\n",
    "        else:\n",
    "            new_attributes = [a for a in attributes if a != best_attribute]\n",
    "            edge.node = id3_gain_ratio(examples_v,\n",
    "                                       target_attribute,\n",
    "                                       new_attributes)\n",
    "    \n",
    "    return root\n",
    "\n",
    "# Growing the tree and printing it.\n",
    "decision_tree_gain_ratio = id3_gain_ratio(data,\n",
    "                                          'poisonous',\n",
    "                                          list(data.drop('poisonous', axis=1).columns))\n",
    "print_tree(decision_tree_gain_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the tree should become bigger than with the information gain selection criterion. To make the tree simpler one could prune the tree.\n",
    "\n",
    "__Task 1.2 (3 points)__ Explain what it means to __pre-prune__ a decision tree and why __pre-pruning__ is applied. Which __pre-pruning method__ would you use? Why would you recommend this method and how does it affect the tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.3 (4 points)__ Now implement one __pre-pruning__ method to your solution from Task 1.1 to give the decision tree pruning capabilities. You may need to change the method signature for this task. Start by copy/pasting your solution from Task 1.1 into the method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3_gain_ratio_and_pruning(examples, target_attribute, attributes):\n",
    "    pass # Copy and paste your solution from the previous task (1.1) and extend/change it :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Hands on with Sklearn (27 points)\n",
    "In this task, we will use sklearn to implement various machine learning algorithms.\n",
    "\n",
    "We will start this task with the __mushroom dataset__, but later on, we will take a look at the __default rate of credit cards__ in a different dataset.\n",
    "\n",
    "The following code imports pandas and sets a random seed variable. This variable is important, as some of the algorithms we will use are based on randomization and by using a random seed we can disable the randomness and get the same results in every run of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Use this random_seed everywhere where you can set a random seed.\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mushroom Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads and quickly inspects the mushroom data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"data/UCI_mushroom_data.csv\", sep=\",\")\n",
    "y_data = data[\"poisonous\"]\n",
    "x_data = data.drop(\"poisonous\", axis=1)\n",
    "\n",
    "feature_names = x_data.columns\n",
    "class_names = y_data.name\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you continue, you need to transform the data, because the sklearn decision tree implementation only works with numerical data. So you need to **convert** all of the **string input** data into **numeric input** data (All columns need to be converted). \n",
    "\n",
    "This conversion can be done in various ways. First, we will use a **Label Encoder**. A label encoder converts every unique string value into a unique integer value. For Example, \"red\" becomes \"1\", \"blue\" becomes \"2\", and \"green\" becomes \"3\".\n",
    "\n",
    "__Task 2.1 (2 points)__ <br>Take a look into sklearn.preprocessing.LabelEncoder on how to use it on your dataset. Remember every categorical (string) column needs to be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.2 (2 points)__ <br>After that, you need to split the dataset into a **Training Set** and a **Test Set**. The test set should contain **40%** of the dataset. Take a look at the sklearn package. Maybe you can find a good utility function to make a split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.3 (1.5 point)__ <br>**Explain** why you need to split your data? Wouldn't it be better to train your model on the whole dataset you have available?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.4 (2 points)__ <br>**Explain** what it means to make a stratified split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to train the decision tree!\n",
    "\n",
    "__Task 2.5 (4 points)__\n",
    "\n",
    "1. Use the sklearn decision tree implemenation to **create** a DecisionTreeClassifier. For the criterion to select the best splitting-feature use the InformationGain (\"entropy\"). Don't forget the random seed.\n",
    "\n",
    "2. **Train** a DecisionTreeClassifier with your **training data**. \n",
    "\n",
    "3. **Verify** the accuracy of your decision tree on your **training data**.\n",
    "\n",
    "4. **Test** the accuracy of your decision tree on previously unseen data with the help of your **test data**. \n",
    "\n",
    "5. **Calculate** the F1-score for the **test data predictions** of your decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1. Create the decision tree classifier\n",
    "\n",
    "\n",
    "# 2. Train (Fit) the decision tree classifier with your training data\n",
    "\n",
    "\n",
    "# 3. Verify training on training data\n",
    "\n",
    "\n",
    "# 4. Test with test data\n",
    "\n",
    "\n",
    "# 5. Calculate F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One big advantage of big frameworks like sklearn is that, a lot of functionality around their classifiers is also implemented. For example, in the case of the decision tree it could be very intersting to see exactly how the decison tree looks.\n",
    "\n",
    "What a coincedence. There is a very handy function implementet in sklearn for that. The only requirement for this is that you also have matplotlib installed.\n",
    "\n",
    "__Task 2.6 (1 point)__ Use **sklearn** to draw your tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "\n",
    "# feature_names and class_names are already defined in the cell where the data is loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! With this function we can see what the decision tree really looks like. Use this representation to answer the following questions:\n",
    "\n",
    "__Task 2.7 (1 point)__ <br>What **feature** is used for the **first split**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.8 (1 point)__ <br>How many **leaf nodes** contain **less** than **100 samples**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.9 (2.5 points)__ <br>\n",
    "**Explain** why OneHotEncoding works closer to real-life than a label encoding, and thus why you should rather use OneHotEncoding over label encoding for categorical data. To answer this question, look closely at **how the sklearn decision tree splits the data** (Look at the decision at each decision-node). **Incorporate** the observed mechanism of sklearn’s decision tree into your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Defaults data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet loads and quickly inspects the new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"data/default_of_credit_card_clients.csv\", sep=\";\")\n",
    "data = data.drop(\"ID\", axis=1)\n",
    "\n",
    "y_data = data[\"default\"]\n",
    "x_data = data.drop(\"default\", axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains categorical and numerical features. You can take a look into the [dataset description](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients).\n",
    "\n",
    "The following code snippet is for your convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all categorical features\n",
    "categorical_columns = [\n",
    "    \"SEX\", \"MARRIAGE\", \"EDUCATION\",\n",
    "    \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"\n",
    "]\n",
    "# List of all numerical features\n",
    "numerical_columns = [\n",
    "    \"LIMIT_BAL\", \"AGE\",\n",
    "    \"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\",\n",
    "    \"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformation and Normalization:\n",
    "\n",
    "Now we again need to transform the features. However, there are categorical and numerical features in our dataset.\n",
    "\n",
    "Of course, we need to transform the categorical features. To do so, we should use OneHotEncoding. \n",
    "\n",
    "But what should we do with the numerical features? The answer is normalization. It's not always necessary to normalize the data, but often it is essential for a good performance of our machine learning models.\n",
    "\n",
    "__Task 2.10 (1.5 points)__ <br> **Explain** why it is a good idea to normalize numerical data for example with a StandardScaler (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.11 (3 points)__<br> Now transform and normalize the features.\n",
    "- Transform the categorical features with OneHotEncoding (the pandas.get_dummies function could be very helpful)\n",
    "- Normalize the numerical features with a StandardScaler (sklearn.preprocessing.StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, split your data into a training and a test set like you did in the previous taks. (You could reuse your code from 2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from Task 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "__Task 2.12 (0.5 points)__ <br>Reuse your sklearn decision tree code here on the credit card data. (Creating, Training, Verifying, Testing, F1-Score of the test predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "__Task 2.13 (2 points)__ \n",
    "\n",
    "1. Use the sklearn **random forest** implemenation to **create** a RandomForestClassifier. For the criterion to select the best splitting-feature use the InformationGain (\"entropy\"). Don't forget the random seed.\n",
    "\n",
    "2. **Train**  your random forest with your **training data**. \n",
    "\n",
    "2. **Verify** the accuracy of your random forest on your **training data**.\n",
    "\n",
    "3. **Test** the accuracy of your random forest on previously unseen data with the help of your **test data**. \n",
    "\n",
    "4. **Calculate** the F1-score for the **test data predictions** of your random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "__Task 2.14 (2 points)__ \n",
    "\n",
    "1. Use the **sklearn perceptron** implemenation to **create** a Perceptron. The stopping criterion (tol) should be 0.001. Don't forget the random seed. You can play around with the parameters.\n",
    "\n",
    "2. **Train** your perceptron with your **training data**.\n",
    "\n",
    "3. **Verify** the accuracy of your perceptron on your **training data**.\n",
    "\n",
    "4. **Test** the accuracy of your perceptron on previously unseen data with the help of your **test data**. \n",
    "\n",
    "5. **Calculate** the F1-score for the **test data predictions** of your perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "__Task 2.15 (1 points)__ Interpret the results. What model performed best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Deep Learning (19 points)\n",
    "\n",
    "We now want to train a Deep Learning model on an image classification task. For this we will use the framework *Keras*. Keras is a high-level neural network framework that allows straight-forward implementation and experimentation with networks that contain arbitrary layers. \n",
    "\n",
    "The framework is embedded in Google's efficient computation framework TensorFlow. So in order to use it, you have to install TensorFlow in the same conda environment you use to open this notebook. You can install it using the command __conda install tensorflow__. If you feel adventurous and have a GPU, you can also install a TensorFlow version with GPU support. For more information on that, see [here](https://www.tensorflow.org/install/gpu).\n",
    "\n",
    "__Note that GPU support is NOT necessary to solve this task.__ The setup for CUDA and similar libraries can be complicated and lead to very device-specific problems. If you already have CUDA installed and you are getting errors, see the tips and tricks section point 2), there is one common issue we can provide a fix for. \n",
    "\n",
    "After installation you can get familiar with the overall workflow in Keras by\n",
    "reading [this ressource](https://keras.io/guides/sequential_model/).\n",
    "\n",
    "Feel free to use other tutorials or installation guides. There are many high quality guides freely available online, which contain specific instructions and walkthroughs for your setup and OS. Please contact us if you need any help with that.\n",
    "\n",
    "We will implement a rather simple convolutional neural network that tackles the task of classifying input images into pre-defined classes. The dataset we\n",
    "are going to use for this assignment is called *Fashion-MNIST*. It consists of\n",
    "low-resolution images of 10 clothing categories.\n",
    "\n",
    "Let us first obtain the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training/test images and labels are now loaded. Time to inspect the dataset a little:\n",
    "\n",
    "The label-to-category matching is the following:\n",
    "\n",
    "| Label   | Description |\n",
    "|:-------:|:------------|\n",
    "| 0 | T-Shirt/Top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "Suppose you don't know what an ankle boot looks like (I had to google it ;).\n",
    "\n",
    "__Task 3.1 (2 points)__ Find the very first image of an ankle boot in the loaded training dataset and show it with the visualization package of your choice. Also find out what the dimensions (width, height) of the images are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.2 (2 points)__ In order to use the images in a Convolutional Neural Network, we have to think about the input dimensions of our images. Along with the width and height of an image, we also need a color channel. The images are all grayscale, thus having only one value for every pixel in their 2D array. But Keras needs an explicit dimension expansion to incorporate a single color channel. This should make the current 2D array which represents one image (height x width) a 3D array (height x width x color). Perform this dimension expansion for your training and test images.\n",
    "\n",
    "Additionally a CNN performs better with small floating point values for the pixel data. Therefore make sure that all your pixel values are normalized to fall in the range between 0.0 and 255.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.3 (2 points)__\n",
    "\n",
    "When you have a look at the given labels for the dataset you see that they are just single integers for the classes. CNNs can not work very well with such labels since the ordinal scale of the classes does not make sense (A T-Shirt can not be put in a meaningful order with a Trouser). Also in the end we want to have the same number of neurons as there are classes so that we can get estimators for each class for a particular instance. The solution here is called: One-hot encoding. So for the first point of this task, please describe in your own words what a one-hot encoding does to your integer labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the second point, perform a one-hot encoding for your train and test labels.\n",
    "\n",
    "__Hint__: The __utils__ package of Keras will help you greatly with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.4 (2 points)__ Since our dataset only has the differentiation between a training and a test set, you will now have to manually do a validation split. Take the test data (images and labels) and divide them in half to receive a validation and a test set. The pairing of image and label is given by the order of the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.5 (5 points)__ Now let us implement our own Convolutional Neural Network architecture with Keras. The structure has to be the following:\n",
    "\n",
    "* Input layer that allows for the input of images preprocessed like above\n",
    "* Convolution layer with 32 filters and a 3x3 weight kernel. Activation: ReLU\n",
    "* Maxpooling layer using a 2x2 window\n",
    "* Convolution layer with 64 filters and a 3x3 weight kernel. Activation: ReLU\n",
    "* Maxpooling layer using a 2x2 window\n",
    "* Flatten Layer\n",
    "* Dense layer with 100 neurons and ReLU activation function\n",
    "* Dropout layer with dropout rate 0.5\n",
    "* Output layer with the number of output classes and Softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.6 (1 point)__ In order for the model to be trainable by Keras, we have to compile it. During compilation, several hyperparameters will also be given by the user. Compile your model to be trained with the optimizer strategy *Stochastic Gradient Descent*, a learning rate of 0.01 and a *momentum* of 0.9. Also find out what kind of loss you need for the output of your particular model and use that. As metrics, we want to have the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.7 (1 points)__ Describe now, in your own words what momentum is in the context of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.8 (2 points)__ We now want to train our model for 10 epochs with the training data and validate it simultaneously with our validation data. How is this done in Keras?\n",
    "\n",
    "Also we want to include a technique called *Early stopping*. Read about what that does and include it in your training. We want to monitor the validation loss and have a patience of 3. Also the best weights should be restored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.9 (1 point)__ Now shortly explain how Early Stopping can help to find better models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.10 (1 point)__ Finally, let us make use of the left out test data and evaluate our model again on the test data. Is there a sign of overfitting with respect to the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tips and tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) CUDA issues\n",
    "One rather common error message concerns a problem with 'cudnn', and a message that the convolution could not be found: \"Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\"   \n",
    "In that case, copy the following code, insert it in the first code field after the description of task 3 begins, remove the '*#*' from all lines, and execute that code field again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.compat.v1 import ConfigProto\n",
    "#from tensorflow.compat.v1 import InteractiveSession\n",
    "#config = ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using CUDA with an NVidia RTX 3000-Series GPU, pay very close attention to the \"Hardware Requirements\" points on [here](https://www.tensorflow.org/install/gpu). Also notice which Tensorflow version you are using, as the older ones may not be compatible with you new GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Embedding images\n",
    "You can embed images in a jupyter notebook on two ways: <br/>\n",
    "First, you can use the IPython kernel to draw an image everytime the code cell is run like shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/logo.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, you can embed images directly in a Markdown cell as shown below. You can either use markdown syntax or write plain HTML code. Sometimes HTML code is more practical, as you have much finer control over the HTML elements.\n",
    "\n",
    "1. Markdown syntax:\n",
    "![title](images/logo.png)\n",
    "2. HTML syntax\n",
    "<img src=\"images/logo.png\" style=\"width: 70px;\"/>\n",
    "\n",
    "If you are having trouble with **markdown images not refreshing after you change them on disk** you need to refresh your browser. The browser chaches images and the old image is still in the cache."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
